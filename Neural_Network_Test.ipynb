{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgqfkzlPWG00PSAt07aXOH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandesai/SciforTechnologies/blob/main/Neural_Network_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Explain the architecture of Spark.\n",
        "- Driver Program in the Apache Spark architecture calls the main program of an application and creates SparkContext. A SparkContext consists of all the basic functionalities. Spark Driver contains various other components such as DAG Scheduler, Task Scheduler, Backend Scheduler, and Block Manager, which are responsible for translating the user-written code into jobs that are actually executed on the cluster.\n",
        "\n",
        "\n",
        "- Spark Driver and SparkContext collectively watch over the job execution within the cluster. Spark Driver works with the Cluster Manager to manage various other jobs. The cluster Manager does the resource allocating work. And then, the job is split into multiple smaller tasks which are further distributed to worker nodes.\n",
        "\n",
        "\n",
        "- Whenever an RDD is created in the SparkContext, it can be distributed across many worker nodes and can also be cached there.\n",
        "\n",
        "\n",
        "- Worker nodes execute the tasks assigned by the Cluster Manager and return it back to the Spark Context.\n",
        "\n",
        "\n",
        "- An executor is responsible for the execution of these tasks. The lifetime of executors is the same as that of the Spark Application. If we want to increase the performance of the system, we can increase the number of workers so that the jobs can be divided into more logical portions."
      ],
      "metadata": {
        "id": "rng_VwIfqKIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain activation function.\n",
        "\n",
        "- In artificial neural networks, an activation function is one that outputs a smaller value for tiny inputs and a higher value if its inputs are greater than a threshold. An activation function \"fires\" if the inputs are big enough; otherwise, nothing happens. An activation function, then, is a gate that verifies how an incoming value is higher than a threshold value.\n",
        "\n",
        "- Because they introduce non-linearities in neural networks and enable the neural networks can learn powerful operations, activation functions are helpful. A feedforward neural network might be refactored into a straightforward linear function or matrix transformation on to its input if indeed the activation functions were taken out."
      ],
      "metadata": {
        "id": "jv19aCgXs2pB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List different types of activation function with their formula.\n",
        "\n",
        "- There are various types of activation function.\n",
        "\n",
        "- Sigmoid: The sigmoid function is known as the logistic function, maps the input to a range between 0 and 1.\n",
        "\tSIGMA(X) = (1 / ( 1 + e^(-x) ) )\n",
        "\n",
        "- Hyperbolic Tangent (tanh): Similar to the sigmoid function, but it maps inputs to a range between -1 and 1. Tanh addresses some of the vanishing gradient issues of the sigmoid function and is often used in hidden layers of neural networks.\n",
        "\ttanh(x) = ( e^(x) - e^(-x) ) / ( e^(x) + e^(-x) )\n",
        "\n",
        "- Rectified Linear Unit (ReLU): ReLU is the most widely used activation function in deep learning. It returns zero for negative inputs and the input value for positive inputs, defined as\n",
        "ReLU(x)=max(0,x).\n",
        "\n",
        "- Leaky ReLU: Leaky ReLU is a variant of ReLU that allows a small, non-zero gradient when the input is negative, which helps prevent dying ReLU neurons.\n",
        "\tLeakyReLU(x) = {x, if x>0     alpha*x otherwise,  where alpha is small constant\n",
        "\n",
        "- - Exponential Linear Unit (ELU): ELU is another variant of ReLU that allows negative values and has smoother gradients.\n",
        "\tELU(x) = {x,   if x>0     alpha * (e^x - 1)   otherwise\n",
        "\n",
        "- Softmax: The softmax activation function transforms the raw outputs of the neural network into a vector of probabilities, essentially a probability distribution over the input classes.\n",
        "\tSoftmax(x) = e^x / SUM(j=1 to n) e^x\n"
      ],
      "metadata": {
        "id": "BXK6qlK8tGaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain Hybrid Inheritance with Code.\n",
        "\n",
        "- Hybrid inheritance is a combination of two or more types of inheritance, often including both single and multiple inheritance. This allows a class to inherit properties and behavior from multiple base classes."
      ],
      "metadata": {
        "id": "OvCohhJAoU_n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEffIMMUnv1t",
        "outputId": "6031734a-c66c-4ba0-e422-2fadedb6be9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brand: Toyota, Model: Corolla, Doors: 4\n",
            "Brand: Toyota, Model: Prius, Doors: 4, Horsepower: 98hp, Battery Capacity: 5kWh\n"
          ]
        }
      ],
      "source": [
        "class Vehicle:\n",
        "    def __init__(self, brand, model):\n",
        "        self.brand = brand\n",
        "        self.model = model\n",
        "\n",
        "    def display_info(self):\n",
        "        return f\"Brand: {self.brand}, Model: {self.model}\"\n",
        "\n",
        "class Engine:\n",
        "    def __init__(self, horsepower):\n",
        "        self.horsepower = horsepower\n",
        "\n",
        "    def show_power(self):\n",
        "        return f\"Horsepower: {self.horsepower}hp\"\n",
        "\n",
        "\n",
        "class Car(Vehicle):\n",
        "    def __init__(self, brand, model, doors):\n",
        "        super().__init__(brand, model)\n",
        "        self.doors = doors\n",
        "\n",
        "    def display_info(self):\n",
        "        return f\"{super().display_info()}, Doors: {self.doors}\"\n",
        "\n",
        "\n",
        "class HybridCar(Vehicle, Engine):\n",
        "    def __init__(self, brand, model, doors, horsepower, battery_capacity):\n",
        "        Vehicle.__init__(self, brand, model)\n",
        "        Engine.__init__(self, horsepower)\n",
        "        self.doors = doors\n",
        "        self.battery_capacity = battery_capacity\n",
        "\n",
        "    def display_info(self):\n",
        "        return f\"{super().display_info()}, Doors: {self.doors}, {super().show_power()}, Battery Capacity: {self.battery_capacity}kWh\"\n",
        "\n",
        "\n",
        "car1 = Car(\"Toyota\", \"Corolla\", 4)\n",
        "print(car1.display_info())\n",
        "\n",
        "hybrid_car = HybridCar(\"Toyota\", \"Prius\", 4, 98, 5)\n",
        "print(hybrid_car.display_info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain Neural Networks.\n",
        "- A neural network is a machine learning model that is developed to mimic the human brain.\n",
        "- It consists of neurons which are the core building blocks of for a neural network.\n",
        "- The simplest type of neural network is a perceptron which consists of a single layer of neurons.\n",
        "\n",
        "- Neural networks are of various types:\n",
        "1. Feedforward neural networks\n",
        "2. ⁠Convolutional neural networks\n",
        "3. ⁠Recurrent neural networks\n",
        "\n",
        "- Components of neural network:\n",
        "1. Neurons\n",
        "2. ⁠Input layer\n",
        "3. ⁠weights and connections\n",
        "4. ⁠Hidden layers\n",
        "5. ⁠output layer\n",
        "\n",
        "\n",
        "- Working:\n",
        "Consider a neural network for classifying fake news. The input layer takes the input features such as news title, author, article description. These inputs are multiplied with weights and passed through hidden layers. The network through training will recognize patterns and clasify the news as fake or not. The output layer with a sigmoid activation function will return the output as 0 or 1. The networks refines itself through a method called as backpropagation.\n"
      ],
      "metadata": {
        "id": "CFFf72cRtnXm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLgaWqDWn5Y8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}